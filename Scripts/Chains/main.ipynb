{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from langchain.chat_models import AzureChatOpenAI\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "api_key = os.getenv('OPENAI_API_KEY')\n",
    "model = AzureChatOpenAI(temperature=0, deployment_name=\"chat\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cadenas de utilidad\n",
    "* Son funciones que ya tienne un proposito muy especifico; generar un resumen a partir de textos, resolver preguntas, crear una conversación con memoria o sin memoria.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The article examines the development of autonomous agents using large language models (LLMs). It covers the different aspects of these agents, such as planning, memory, and tool use. The article presents case studies and examples of LLM-powered agents in different fields, including science and simulation. It also addresses the challenges and limitations associated with using LLMs in autonomous agents.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load_summarize_chain\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain_community.chat_models import ChatOpenAI\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "# Cargamos el pdf\n",
    "loader = WebBaseLoader(\"https://lilianweng.github.io/posts/2023-06-23-agent/\")\n",
    "docs = loader.load()\n",
    "\n",
    "\n",
    "chain = load_summarize_chain(model, chain_type=\"map_reduce\", verbose=False)  #stuff o map_reduce\n",
    "\n",
    "chain.run(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El artículo discute la idea de construir agentes autónomos utilizando modelos de lenguaje grandes (LLM) como controladores principales. Se presentan varios ejemplos de demostraciones de concepto, como AutoGPT y GPT-Engineer, que demuestran el potencial de los LLM para resolver problemas complejos. El sistema de agente autónomo consta de tres componentes principales: planificación, memoria y uso de herramientas. La planificación implica descomponer tareas complejas en subobjetivos más pequeños y mejorar los resultados a través de la autorreflexión. La memoria incluye la memoria a corto plazo y la memoria a largo plazo, que permite al agente retener y recuperar información. El uso de herramientas implica que el agente aprenda a utilizar API externas para obtener información adicional. Sin embargo, existen desafíos en cuanto a la longitud limitada del contexto, la planificación a largo plazo y la confiabilidad de la interfaz de lenguaje natural.\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains.combine_documents.stuff import StuffDocumentsChain\n",
    "from langchain.chains.llm import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt_template = \"\"\"Write a concise summary of the following:\n",
    "\"{text}\"\n",
    "CONCISE SUMMARY in spanish:\"\"\"\n",
    "prompt = PromptTemplate.from_template(prompt_template)\n",
    "llm_chain = LLMChain(llm=model, prompt=prompt)\n",
    "stuff_chain = StuffDocumentsChain(llm_chain=llm_chain, document_variable_name=\"text\")\n",
    "print(stuff_chain.run(docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pendiente a ver si funciona\n",
    "# from langchain.chains import (\n",
    "#     StuffDocumentsChain,\n",
    "#     LLMChain,\n",
    "#     ReduceDocumentsChain,\n",
    "#     MapReduceDocumentsChain,\n",
    "# )\n",
    "# from langchain_core.prompts import PromptTemplate\n",
    "# from langchain_community.llms import OpenAI\n",
    "\n",
    "# # This controls how each document will be formatted. Specifically,\n",
    "# # it will be passed to `format_document` - see that function for more\n",
    "# # details.\n",
    "# document_prompt = PromptTemplate(\n",
    "#     input_variables=[\"page_content\"],\n",
    "#      template=\"{page_content}\"\n",
    "# )\n",
    "# document_variable_name = \"context\"\n",
    "# llm = model\n",
    "# # The prompt here should take as an input variable the\n",
    "# # `document_variable_name`\n",
    "# prompt = PromptTemplate.from_template(\n",
    "#     \"Summarize this content: {context}\"\n",
    "# )\n",
    "# llm_chain = LLMChain(llm=llm, prompt=prompt)\n",
    "# # We now define how to combine these summaries\n",
    "# reduce_prompt = PromptTemplate.from_template(\n",
    "#     \"Combine these summaries: {context}\"\n",
    "# )\n",
    "# reduce_llm_chain = LLMChain(llm=llm, prompt=reduce_prompt)\n",
    "# combine_documents_chain = StuffDocumentsChain(\n",
    "#     llm_chain=reduce_llm_chain,\n",
    "#     document_prompt=document_prompt,\n",
    "#     document_variable_name=document_variable_name\n",
    "# )\n",
    "# reduce_documents_chain = ReduceDocumentsChain(\n",
    "#     combine_documents_chain=combine_documents_chain,\n",
    "# )\n",
    "# chain = MapReduceDocumentsChain(\n",
    "#     llm_chain=llm_chain,\n",
    "#     reduce_documents_chain=reduce_documents_chain,\n",
    "# )\n",
    "# # If we wanted to, we could also pass in collapse_documents_chain\n",
    "# # which is specifically aimed at collapsing documents BEFORE\n",
    "# # the final call.\n",
    "# prompt = PromptTemplate.from_template(\n",
    "#     \"Collapse this content: {context}\"\n",
    "# )\n",
    "# llm_chain = LLMChain(llm=llm, prompt=prompt)\n",
    "# collapse_documents_chain = StuffDocumentsChain(\n",
    "#     llm_chain=llm_chain,\n",
    "#     document_prompt=document_prompt,\n",
    "#     document_variable_name=document_variable_name\n",
    "# )\n",
    "# reduce_documents_chain = ReduceDocumentsChain(\n",
    "#     combine_documents_chain=combine_documents_chain,\n",
    "#     collapse_documents_chain=collapse_documents_chain,\n",
    "# )\n",
    "# chain = MapReduceDocumentsChain(\n",
    "#     llm_chain=llm_chain,\n",
    "#     reduce_documents_chain=reduce_documents_chain,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chain.run(docs[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
